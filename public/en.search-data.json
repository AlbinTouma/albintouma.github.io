{"/posts/":{"data":{"":" RSS Feed "},"title":"Archive"},"/posts/enrichment/":{"data":{"#":"Business Problem.When I set up Know Your Sanctions I wanted users to both identify political actors in the UK and their financial interests. By combining the UK Parliament’s list of MPs with the Register of Members Financial Interest dataset, I was able to enrich the data on politicians.\nCollect PEP dataRequest data with UK Parliament’s API When gathering data on politically exposed persons, a good place to start is always the country’s legislative assembly. Lucky me, Parliament has an API that makes gathering basic data on members of the House of Commons and the House of Lords easier.\nI know from the documentation that there are 1400 entries and set up my parameters:\nbase_url = 'https://members-api.parliament.uk/api/Members/Search' skip = 0 take = 20 total_entries = 1400 all_members = [] We gather our data in a a while statement that sends an API request until all 1400 entries have been collected.\nIf we recieve a valid response, we save the data from each json response to my members varaible that is then appended to my all_members list. Skip is then updated for the next request so that we know when we reach our last request.\nwhile skip \u003c total_entries: # Make a request to the API with the current skip value url = f'{base_url}?skip={skip}\u0026take={take}' response = requests.get(url) if response.status_code == 200: try: data = response.json() members = data['items'] # Append the retrieved members to the list all_members.extend(members) # Update the skip value for the next request skip += take except (json.JSONDecodeError, KeyError) as e: print(\"Error parsing JSON:\", str(e)) break else: print(\"Error fetching data. Status code:\", response.status_code) break Extract the Data Our data is now stored in our all_members list. This list is a list of dictionaries.\n[{'value': {'id': 172, 'nameListAs': 'Abbott, Ms Diane', 'nameDisplayAs': 'Ms Diane Abbott', 'nameFullTitle': 'Rt Hon Diane Abbott MP', 'nameAddressAs': 'Ms Abbott', 'latestParty': {'id': 8, 'name': 'Independent', 'abbreviation': 'Ind', 'backgroundColour': 'C0C0C0', 'foregroundColour': 'FFFFFF', 'isLordsMainParty': False, 'isLordsSpiritualParty': False, 'governmentType': None, 'isIndependentParty': True}, {'rel': 'contactInformation', 'href': '/Members/331/Contact', 'method': 'GET'}]}, {'value': {'id': 1615, 'nameListAs': 'Barnes, Michael', 'nameDisplayAs': 'Michael Barnes', 'nameFullTitle': 'Michael Barnes', 'nameAddressAs': None, 'latestParty': {'id': 15, 'name': 'Labour', 'abbreviation': 'Lab', 'backgroundColour': 'ff0000', 'foregroundColour': 'ffffff', 'isLordsMainParty': True, 'isLordsSpiritualParty': False, 'governmentType': 3, 'isIndependentParty': False}, 'gender': 'M', 'latestHouseMembership': {'membershipFrom': 'Brentford and Chiswick', 'membershipFromId': 430, 'house': 1, 'membershipStartDate': '1966-03-31T00:00:00', 'membershipEndDate': '1974-02-28T00:00:00', 'membershipEndReason': None, 'membershipEndReasonNotes': None, 'membershipEndReasonId': None, 'membershipStatus': None}, 'thumbnailUrl': 'https://members-api.parliament.uk/api/Members/1615/Thumbnail'}, 'links': [{'rel': 'self', 'href': '/Members/1615', 'method': 'GET'}, {'rel': 'overview', 'href': '/Members/1615', 'method': 'GET'}, {'rel': 'synopsis', 'href': '/Members/1615/Synopsis', 'method': 'GET'}, {'rel': 'contactInformation', 'href': '/Members/1615/Contact', 'method': 'GET'}]}, We extract the data for each member by looping over the list of dictionaries and appending the item to each list. If no item is found, we’ll simply append an empty string.\n# Extracting data for each member for member in all_members: try: id.append(member['value']['id']) except: id.append('') ...etc Now that we have each parliamentarian’s data, we can create a data frame and save the result in a csv.\ndf = pd.DataFrame({ 'entity_id': id, \"Name\": names, \"Party\": parties, \"Membership\": memberships, \"start_date\": start_dates, \"end_date\": end_dates, \"active\": statuses, 'status_description': status_description, 'house': house }) df.to_csv('./UK Parliament/parliamentarians.csv') Enrich PEP dataThe Register of Members Financial Interest Details about the financial interests of Members of the House of Commons and the House of Lords is available as a csv on Members Interest. What’s great about Members Interest is that they have indexed the Register of Members Financial Interests.\nThe main purpose of the Register is to provide information about any financial interest which a Member has, or any benefit which he or she receives, which others might reasonably consider to influence his or her actions or words as a Member of Parliament.\nWhy use the Register of Members Financial Interest? The Register of Members Financial Interest is really useful and contains:\nWho donors are and how much they donated The Employment and earnings of peers Visits outside the UK made by peers, to whom they travelled, where they stayed, costs and the purpose of their visit Miscellaneous data on gifts and hospitality With these data points, we can work out the habits and connections of our politically exposed person. It tells us about their employment, salary, who they worked for, where they’ve been, who donates to whom and why and why our MPs go abroad.\nWhat’s great is that the register is updated every 28 days and it goes back many years, meaning we have a good understanding of the different interests that our peers have.\nHow to enrich our PEP data? Since the financial interest data comes as a csv, all we need to do is download the csv and merge the data with our file on MPs and Lords in order to enrich our PEP profiles.","business-problem#Business Problem.":"","collect-pep-data#Collect PEP data":"","enrich-pep-data#Enrich PEP data":""},"title":"The financial interests of British politicians - a case study in enriching PEP data"},"/posts/excel/":{"data":{"#":"Business Problem.As an analyst I recieve monthly excel sheets from our data vendor that describes their data coverage. My job is to analyse changes in their coverage. This usually takes about a day to do, so I decided to save myself the effort with a python programme that lets anyone in the company complete this task.\nEvery project needs a good definition of done, and I was able to find a good definition by thinking about what I wanted the user to achieve:\n“..the user should load the file, run the programme, and then see a heatmap of our vendor’s data coverage and a list of changes in coverage compared to last month’s file.”\nDone is then when the python programme:\nScores the input coverage Excel sheet and saves the output as a heatmap Compares the heatmap to the previous month’s heatmap and logs any differences How Excel let us down Picture this: each month, the data vendor sends you a file of their current data coverage. Our product is based on this data, so it’s crucial that engineers, sales, and product owners know about any updates with speed and high accuracy.\nKnowing about our new officer data in France before that morning coffee with a potential client can really be the difference between deal and no deal, and with Excel, we risked no deal.\nThe reason why is because the task of manually scoring and comparing files was too tedious. Not only that, Excel would often crash.\nScoring the sheets required long if statements and XLOOKUP formulas that often broke down when trying to transfer scores from the data vendor’s list of countries and states to our internal list of all the world’s countries and states.\nIf that doesn’t sound bad, wait until you hear about the next step. To find changes, you had to copy the last months’ results and paste it into this months’ sheet. Then, you had to compare cells to identify differences before filtering booleans to find changes. Not only was this prone to human error, there was never the time to compare multiple months at once.\nAs soon as you tried lookups or any other solution in to speed up the process, Excel would simply crash.\nWith Python, I increased the efficiency and accuracy of comparing our data vendor’s coverage sheets and transformed the process from a complicated set of tasks to a programme that anyone in the company can run with a simply ‘python main.py’ in the command line.","business-problem#Business Problem.":"","from-business-problem-to-solution#From business problem to solution":"I said earlier that ‘knowing about updates in our data before that morning coffee with a potential client can be the difference between deal and no deal’.\nWith this Python programme, I can now update our sales team about changes in our data in less than the time it takes them to finish their coffee.","objective-1-score-excel-files-and-save-output#Objective 1: Score Excel files and save output":"Cleaning and scoring the vendor’s data files is our first step. Our for loop applies the different steps in the process to each file.\nmain.py # Loop through the input directory and score each OC coverage sheet. for input_file in input_files: input_file_path = os.path.join(input_directory, input_file) data = load_input(input_file_path) file_date = extract_date(input_file_path) oc_coverage(data, file_date) 1) Load data The loop begins by saving the path of each file and passing the path to our load_data function.\nload_files.py def load_input(file_path): data = pd.read_excel(file_path, sheet_name='Coverage Overview', skiprows=3) return data 2) Extract date After loading the data, we extract the date when the file was last updated by the vendor so that we can organise the Excel sheets and identify any changes in their data.\nWe find when the last file was last updated at C2. To extract this date, we call the extract_date function.\nfile_date.py def extract_date(file_path): workbook = load_workbook(filename=file_path) sheet = workbook['Coverage Overview'] date = sheet['C1'].value date = datetime.strptime(date, \"%dth %B %Y\") date = date.strftime(\"%dth-%B-%Y\") return date The extract_date function loads the workbook and sheet where the date is found and saves the date as dd-mm-yy.\n3) Score Excel Sheet Now that we’ve extracted both the data and the last updated date, we can call the oc_coverage function. This function cleans, scores, and saves the output of the Excel files as heatmaps organised by month in the output folder.\nWe first remove unwanted columns; these are the blank columns containing Unnamed. Columns refering to the freshness of data is converted by mapping the categorical data to corresponding values in our freshness_dictionary. We keep a copy of this dictionary in our lookup folder where we also keep a list of all of the states and countries in the world.\nThe next step is to score the data. We assign 10 where there is firmographic and officer data and 0 where no data exists. The result is a dataframe with converted freshness and scores showing the states and countries where our vendor has firmograhpic and officer data.\nOnce scored, we import our csv with the world’s countries and states and perform an outer merge on our data frame and fill na values with 0.\ndef oc_coverage(data, date): date = date # Import data \u0026 remove empty columns data = data.loc[:, ~data.columns.str.contains('Unnamed')] data = data.copy() # Convert freshness columns to integers for col in ['Cadence (Updated to OC)', 'Cadence (Incorporation to OC Entry)']: data[col] = data[col].map(freshness_dictionary) # Compute and map freshness id to Freshness dictionary data.loc[:, 'Freshness'] = data.apply(lambda row: ( row['Cadence (Incorporation to OC Entry)'] + row['Cadence (Updated to OC)']) / 2, axis=1).round(0) data.loc[:, 'Freshness'] = data['Freshness'].map( {v: k for k, v in freshness_dictionary.items()}) # Score firmographic and officer data.loc[:, 'Firmographic'] = data.apply( lambda row: 10 if row['Legal name'] == 1 else 0, axis=1) data.loc[:, 'Officer'] = data.apply( lambda row: 0 if row['Officer Count'] == 'No Officers' else 10, axis=1) # Select only the relevant columns data = data[['Jurisdiction', 'Freshness', 'Firmographic', 'Officer']] data = data # Import jurisdictions lookup table and merge data with lookup table jurisdiction_table = pd.read_csv( 'scripts/lookup/template_jurisdictions.csv') oc_coverage = data.merge( jurisdiction_table, how='outer', on='Jurisdiction') # Assign na values for fresshness, officer, and firmographics oc_coverage['Freshness'] = oc_coverage['Freshness'].apply( lambda value: 'Not Covered' if pd.isna(value) else value) oc_coverage[['Firmographic', 'Officer']] = oc_coverage[[ 'Firmographic', 'Officer']].fillna(0) # Export coverage sheet as excel file and append date of file to the name. excel_file_path = f'./output/coverage_sheets/{date}.xlsx' oc_coverage.to_excel(excel_file_path, index=False) The result of merging the data frames is a heatmap of the countries in the world where our data vendor has data. We then save this heatmap in our output directory and append the date when the file was last updated to the name of the file.\nInscoring the Excel sheets, we’ve loaded the data, extracted the date, scored the files and saved the output of each file as heatmaps that can be organised by the date when they were last updated. This completes the first task of the programme.\nThe next step is to let the user compare different months and see the latest changes in our vendor’s data.","objective-2-compare-files-and-write-changes-to-a-log#Objective 2: Compare files and write changes to a log":"The second step is to compare the heatmaps and identify any differences between the latest update from our data vendor and last month’s file. In our main.py file, we loop through our heatmaps stored in our output directory.\nThe loop below loads the output files and adds the date when they were last updated as a column to each file before appending the files to a list.\nfiles = [] merged_data = pd.DataFrame() # Loop through the output directory to compare each OC coverage sheet for output_file in output_files: output_file_path = os.path.join( output_coverage_sheets_directory, output_file) output_data = load_output(output_file_path) output_data = add_date_columns(output_file, output_data) files.append(output_data) merged_data = merge_dfs(files) change_log(merged_data) The heatmap files no longer have cell C3 where the date used to be, so we extract it from the name of the file instead by calling the add_date_columns function.\nfile_date.py #Takes the date in the heatmap filename and populates a column in heatmap with this date. def add_date_columns(output_file, output_data): filename_date_str = output_file filename_date = parser.parse(filename_date_str, fuzzy=True) month_name = filename_date.strftime('%b') output_data['Date'] = month_name return output_data The add_date_columns function takes the date in the heatmap’s filename and uses the date to populate a column in the heatmap. This is useful because we’ll merge the files to create a big datafraem and the dates will act as a unique identifier.\nThe main.py then passes the list of files to our merger function, which essentially selects relevant columns and performs an outer merge on those columns.\nmerge.py #Merge all of the OC heatmaps def merge_dfs(files): merge_columns = ['Jurisdiction', 'Country', 'Firmographic', 'Officer', 'Freshness', 'Category', 'ISO_2', 'iso_2', 'Region', 'Date'] merged_data = reduce(lambda left, right: pd.merge( left, right, on=merge_columns, how='outer'), files) return merged_data With the returned dataframe called merged_data, we can call our function change_log in main.py to compare heatmaps and register changes in a log.\nCompare files and log changes The change_log function compares the different heatmaps and logs changes by month and in a mster_change log that contains changes across all months.\nThe function starts by creating a pivot table from the merged data frames. It then pivots the table by date. The result is a multi-indexed dataframe with hierarchical data.\nWe then create variables to compare the months; column 0 is always the value of the latest month and column 1 is the value of the second month we want to compare.\nWith these variables, we can compare the months. If it is true that firmographic 1 is the same as firmographi2, then True is inserted into the no change column. The same goes for all columns.\nscoring_data.py def change_log(merged_df): change_log = pd.pivot_table(merged_df, index=[ 'Jurisdiction', 'Date', 'Freshness', 'Firmographic', 'Officer']).reset_index() change_log = change_log.pivot(index='Jurisdiction', columns='Date', values=[ 'Firmographic', 'Officer', 'Freshness']) firmographic1 = change_log['Firmographic'].columns[0] firmographic2 = change_log['Firmographic'].columns[1] officer1 = change_log['Officer'].columns[0] officer2 = change_log['Officer'].columns[1] freshness1 = change_log['Freshness'].columns[0] freshness2 = change_log['Freshness'].columns[1] # Take the first column in Firmographics and compare it to the second. change_log[('No Change', 'Firmographics')] = (change_log[( 'Firmographic', firmographic1)] == change_log[('Firmographic', firmographic2)]) change_log[('No Change', 'Officer')] = ( change_log[('Officer', officer1)] == change_log[('Officer', officer2)]) change_log[('No Change', 'Freshness')] = ( change_log[('Freshness', freshness1)] == change_log[('Freshness', freshness2)]) change_log.to_excel('./output/change_log/master_change_log.xlsx') filter_mask = ( ~change_log[(\"No Change\", \"Firmographics\")] | ~change_log[(\"No Change\", \"Officer\")] | ~change_log[(\"No Change\", \"Freshness\")] ) changes = change_log[filter_mask] changes_cols = [ ('Firmographic', changes['Firmographic'].columns[0]), ('Firmographic', changes['Firmographic'].columns[2]), ('Officer', changes['Officer'].columns[0]), ('Officer', changes['Officer'].columns[2]), ('Freshness', changes['Freshness'].columns[0]), ('Freshness', changes['Freshness'].columns[2]) ] filtered_changes = changes[changes_cols] today_date = datetime.datetime.now().strftime('%Y-%m-%d') if filtered_changes.empty: filtered_changes.to_excel(f'./output/change_log/{today_date}_log.xlsx') print('No jurisdictions with changes found') else: print('Rows with changes') filtered_changes.to_excel(f'./output/change_log/{today_date}_log.xlsx') Once we have booleans to demarkate where there’s been a change in value between different months, we save the output to a master_change_log. This file is updated everytime we run the programme and it contains all the months. Each month when we get a new file from our vendor and run the script, the month will be repesented in a column on this log.\nThe master_change log is helpful for viewing change overtime, but what if we want a quick update that highligths the latest changes so we can tell the team and key stakeholders?\nTo achieve this, we create a mask that lets us filter rows where there’s been a change. This narrows our dataframe down to a short table that we save as an Excel file.\nIf there’s been no changes, that is, if the dataframe is empty, we’ll print a statement that says that there’s been no changes. However, if there’s been a change, we’ll save the filtered dataframe with the changes.\nTo stay organised, we’ll save each change_log with today’s date appended to the end.","python-solution#Python Solution":"The structure of the programme is simple. We have three directories. One folder where we store our scripts, a data input folder for the Excel files from our data vendor, and an output folder where we store the scored Excel files and our changes log.\nThe a main.py file runs each task of our programme. Each task is its own python file. This helps us break down different problems and keep our code organised.\nOur main.py loads both the Excel files from our data vendor and the processed files in order to score and compare sheets.\nmain.py # Load Excel files from input directory and load output_files input_files = [f for f in os.listdir(input_directory) if f.endswith('.xlsx')] output_files = [f for f in os.listdir( output_coverage_sheets_directory) if f.endswith('.xlsx')] "},"title":"Automate Excel with Python"},"/posts/mappingmedia/":{"data":{"":"Recently I was asked to join the adverse media team as their new researcher. While the team had a robust database of news sources, they had no way of knowing what news sources exist in different countries. Not knowing “what’s out there” made it difficult to identify gaps in our data, so I set out to build a knoweldge base of the world’s news sources.","collating-multiple-sources-of-truth-into-a-knowledge-base#Collating multiple sources of truth into a knowledge base":"To estimate media sources in any country, we create a country research sheet on Google Drive and store data from each directory in separate tabs:\nWikipedia : Use DBPedia scraper to extract information from the country’s list of newspapers page. Clean the data to remove irrelevant information, such as ceased publications.\nWorldMap: Run the world newspaper map scraper to collect data from this open-source project.\nOfficial Directories: Add data manually or with a script from press directories.\nCompany Data: Include our existing database in its own tab. Filter by country code and extract our domains for that country.\nOther sources: Add any additional identified sources.","identifying-sources#Identifying sources":"Identifying the world’s news sources is not straightforward. Comprehensive lists of newspapers worldwide do not exist. To estimate available news sources in a country, I developed a research process for collating a comprehensive list of news sources in different countries.\nDBPedia \u0026 Wikipedia Lists Wikipedia is a rich source of information for identifying newspapers in a country. You can use search terms like “list of newspapers in [country]” to find these lists. Wikipedia has two strengths. It usually labels newspapers as national and regional, aiding in labelling easier. Most newspapers also have their own dedicated Wikipedia page with detailed information, such as circulation, owners, and publication status.\nYou can use the DBPedia scraper to extract information about newspapers in a country. Add the name of the country to the query and the script will iterate through all of the hyperlinks on the page and extract information from the information boxes.\nOne thing to keep in mind is that the DBPedia scraper extracts information from all of the hyperlinks on a page, including irrelevant pages like towns or regions. You can filter for “newspaper” type only if you want to avoid cleaning the data. However, filtering reduces the number of hits substantially since DBPedia data is not properly organised. My recommendation is to not filter as the ratio of irrelevant data to newspapers is low.\nOfficial Directories Official directories list registered national and regional newspapers and can be found via Google searcher or on a country’s press ombudsman’s website. While these directories often only provide the name and domain of media sources, their official status make them valuable sources of truth.\nOpen-source projects and Unofficial Directories Several unofficial directories and open-source projects, like WorldMap, contain names and domains of thousands of newspapers worldwide. Additionally, blog posts for language learners and media directories hosted by companies offer useful lists. These sources can supplement official directories, though they may require verification.","labelling-and-collating-sources#Labelling and Collating Sources":"The next step is to manually label sources based on our typology, determining their classification. This process is facilitated by first collating sources and then filtering out those without a taxonomy. Collation involves merging research tabs into a master list that serves as our knowledge base of media sources in a country.","the-result#The Result":"The result is a comprehensive list of the adverse media team’s newspapers and the sources we believe exist in each country. With this collated list, our sales team can identify the number and types of media sources we cover. Internally, we can spot data gaps and review new sources for inclusion. Sources rejected in the review are flagged with reasons for their exclusion, maintaining a clear and up-to-date database."},"title":"Mapping Media - A Qualitative Approach to Estimating Globals News Coverage\""},"/posts/typology/":{"data":{"":"Adverse Media, also known as negative news screening, requires that companies identify risks associated with a company or individual in media. Traditionally, compliance officers use Google and keyword searches to find articles, but this process has largely been automated by machine learning models that can identify risks in thousands of articles online.\nThe explosion in the number of articles that companies process has led to confusion in the compliance industry as to what we mean by “good coverage” when we talk about adverse media. Many fall back on quantity as a measure of good coverage, but this approach is flawed due to the high levels of noise in adverse media. Some sources are simply irrelevant - for example, one provider boasts about scraping 200,000 domains, including recipe blogs. Part of the challenge is the lack of a standard way to describe sources.\nIn this article I’ll share with you how I designed an adverse media typology and why the relevance of sources is more important than their perceived reliability.","best-practices#Best Practices":"The Wolfsberg Group recommends prioritising international, national and regional newspapers and to keep in mind countries with restricted press freedoms. They suggest compliance officers rely on these sources due to their higher quality reporting and editorial oversight, which ensure corroborated and edited content. Regional and local newspapers are also useful for reporting on events not covered by larger outlets but can be less reliable.","impact#Impact":"The typology helps the adverse media team demonstrate the relevance of our sources. There are only so many newspapers published on a national and regional level, and by labelling our sources, it is possible to explain to clients why those specific newspapers were selected and others were excluded. This shifts the conversation from “Do you cover everything” (as if adverse media screening is a tick box exercise)  to “No, we curate a list of relevant sources and won’t to cover everything and anything”. With the typology, we can also show clients how we capture the SOE iceberg by listing the largest and most known media sources and then demonstrating how we also include less well-known media sites.","recap#Recap":"Adverse Media is full of noise, and the industry has yet to agree on what constitutes “good media coverage”. The focus often remains on quantity rather than relevance. With a typology of sources in Adverse Media, we can pivot the conversation and discuss “relevance”. This way, the industry can ask whether an AML provider curates relevant sources.","the-legal-research-behind-the-typology#The legal research behind the typology":"The taxonomy is based on best practices rather than regulatory requirements. From legal research and collaboration with our regulatory affairs team, we found that adverse media regulatory requirements are broad and similar across countries. Regulations do not clearly define what sources AML officers should cover, and clients often struggle to articulate their needs. This broad regulatory landscape allows us the freedom to shape and improve our typology.\nIn the absence of strict regulatory requirements, I turned to industry best practices to define what “good” means in adverse media. One organisation providing such guidance is the Wolfsberg Group.","typology-of-sources-in-adverse-media#Typology of sources in adverse media":"I set out to build a typology to help communicate our coverage and sources to clients. As such, our adverse media typology had to explain why we selected specific sources and not “everything” in order to move the conversation away from “quantity” to relevance and quality.\nThe table below, created through iterations with clients and regulatory affairs, serves as an example of how to define sources in adverse media.\nTypology Description International newspaper News aggregators, online websites, internationally distributed newspapers National newspaper A newspaper distributed in one country on a national level Regional newspaper A newspaper distributed in a city or region Judicial authority A source extracted from a court or judicial ministry Police authority A source extracted from a police department or agency Public authority A source extracted from a public authority or international organisation Industry or professional blog A blog or company website published by a professional or company ","why-the-typology-is-not-a-ranking-of-sources#Why the Typology is Not a Ranking of Sources":"Initially, I considered ranking sources based on reliability, placing public, judicial, and government sources at the top. However, client feedback revealed that prioritising certain sources could lead to an overemphasis on quantity over quality. Clients often asked, “Why can’t we have more judicial sources?” The reality is that there are only so many courts in a country. Instead of ranking sources, we built the taxonomy around the concept of relevance.\nAim for Relevance Rather than Strict Reliability\nThe Wolfsberg Group’s recommendations are concerned with high-ranking sources on Google and emphasise international and national newspapers as “reliable sources”. However, with a machine learning approach, the goal is to cover what is most relevant while also retrieving pages that simple keywords might miss. Think of adverse media as a search-optimised (SEO) iceberg. A well-defined typology helps us demonstrate the relevance of our sources, covering both the visible top-ranking domains and the curated sources below the surface that an AML officer might miss with simple keyword searches."},"title":"Adverse Media Typology"}}